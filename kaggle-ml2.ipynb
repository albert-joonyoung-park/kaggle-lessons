{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-24T20:13:52.895862Z","iopub.execute_input":"2023-07-24T20:13:52.896307Z","iopub.status.idle":"2023-07-24T20:13:52.908714Z","shell.execute_reply.started":"2023-07-24T20:13:52.896272Z","shell.execute_reply":"2023-07-24T20:13:52.907651Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/melbourne-housing-snapshot/melb_data.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Missing values   \n* Drop columns with NaN\n* Imputation\n* Extension to imputation","metadata":{}},{"cell_type":"markdown","source":"**Load modules and data**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load the data\ndata = pd.read_csv('/kaggle/input/melbourne-housing-snapshot/melb_data.csv')","metadata":{"execution":{"iopub.status.busy":"2023-07-24T20:13:52.910693Z","iopub.execute_input":"2023-07-24T20:13:52.911112Z","iopub.status.idle":"2023-07-24T20:13:53.580148Z","shell.execute_reply.started":"2023-07-24T20:13:52.911069Z","shell.execute_reply":"2023-07-24T20:13:53.579062Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# check the data, look for missing values\nprint(data.head())                       # review data, missing values\nprint(\"_\" * 70)\nprint(data.dtypes, type(data.dtypes))    # what are the column types?\nprint(\"_\" * 70)\ndata.dtypes.value_counts()               # column_type - count\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-07-24T20:13:53.581760Z","iopub.execute_input":"2023-07-24T20:13:53.582377Z","iopub.status.idle":"2023-07-24T20:13:53.624810Z","shell.execute_reply.started":"2023-07-24T20:13:53.582344Z","shell.execute_reply":"2023-07-24T20:13:53.623629Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"       Suburb           Address  Rooms Type      Price Method SellerG  \\\n0  Abbotsford      85 Turner St      2    h  1480000.0      S  Biggin   \n1  Abbotsford   25 Bloomburg St      2    h  1035000.0      S  Biggin   \n2  Abbotsford      5 Charles St      3    h  1465000.0     SP  Biggin   \n3  Abbotsford  40 Federation La      3    h   850000.0     PI  Biggin   \n4  Abbotsford       55a Park St      4    h  1600000.0     VB  Nelson   \n\n        Date  Distance  Postcode  ...  Bathroom  Car  Landsize  BuildingArea  \\\n0  3/12/2016       2.5    3067.0  ...       1.0  1.0     202.0           NaN   \n1  4/02/2016       2.5    3067.0  ...       1.0  0.0     156.0          79.0   \n2  4/03/2017       2.5    3067.0  ...       2.0  0.0     134.0         150.0   \n3  4/03/2017       2.5    3067.0  ...       2.0  1.0      94.0           NaN   \n4  4/06/2016       2.5    3067.0  ...       1.0  2.0     120.0         142.0   \n\n   YearBuilt  CouncilArea Lattitude  Longtitude             Regionname  \\\n0        NaN        Yarra  -37.7996    144.9984  Northern Metropolitan   \n1     1900.0        Yarra  -37.8079    144.9934  Northern Metropolitan   \n2     1900.0        Yarra  -37.8093    144.9944  Northern Metropolitan   \n3        NaN        Yarra  -37.7969    144.9969  Northern Metropolitan   \n4     2014.0        Yarra  -37.8072    144.9941  Northern Metropolitan   \n\n  Propertycount  \n0        4019.0  \n1        4019.0  \n2        4019.0  \n3        4019.0  \n4        4019.0  \n\n[5 rows x 21 columns]\n______________________________________________________________________\nSuburb            object\nAddress           object\nRooms              int64\nType              object\nPrice            float64\nMethod            object\nSellerG           object\nDate              object\nDistance         float64\nPostcode         float64\nBedroom2         float64\nBathroom         float64\nCar              float64\nLandsize         float64\nBuildingArea     float64\nYearBuilt        float64\nCouncilArea       object\nLattitude        float64\nLongtitude       float64\nRegionname        object\nPropertycount    float64\ndtype: object <class 'pandas.core.series.Series'>\n______________________________________________________________________\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"float64    12\nobject      8\nint64       1\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"**What is the model going to predict?**","metadata":{}},{"cell_type":"code","source":"# The model will predict the 'Price' of the dataset \ny = data.Price","metadata":{"execution":{"iopub.status.busy":"2023-07-24T20:13:53.626320Z","iopub.execute_input":"2023-07-24T20:13:53.626647Z","iopub.status.idle":"2023-07-24T20:13:53.631388Z","shell.execute_reply.started":"2023-07-24T20:13:53.626620Z","shell.execute_reply":"2023-07-24T20:13:53.630149Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"**Prepare features(predictors)**   \nWill only use numeric type columns as predictors","metadata":{}},{"cell_type":"code","source":"melb_predictors = data.drop(['Price'], axis=1)         # drop column 'Price', being target column\nX = melb_predictors.select_dtypes(exclude=['object'])  # as-per the requirement, keep only numeric / boolean valued columns","metadata":{"execution":{"iopub.status.busy":"2023-07-24T20:13:53.635635Z","iopub.execute_input":"2023-07-24T20:13:53.636390Z","iopub.status.idle":"2023-07-24T20:13:53.650581Z","shell.execute_reply.started":"2023-07-24T20:13:53.636335Z","shell.execute_reply":"2023-07-24T20:13:53.649612Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# check the new filtered data frames with predictors(features)\nX\n# The predictors are ready","metadata":{"execution":{"iopub.status.busy":"2023-07-24T20:13:53.651979Z","iopub.execute_input":"2023-07-24T20:13:53.652386Z","iopub.status.idle":"2023-07-24T20:13:53.688984Z","shell.execute_reply.started":"2023-07-24T20:13:53.652357Z","shell.execute_reply":"2023-07-24T20:13:53.687817Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"       Rooms  Distance  Postcode  Bedroom2  Bathroom  Car  Landsize  \\\n0          2       2.5    3067.0       2.0       1.0  1.0     202.0   \n1          2       2.5    3067.0       2.0       1.0  0.0     156.0   \n2          3       2.5    3067.0       3.0       2.0  0.0     134.0   \n3          3       2.5    3067.0       3.0       2.0  1.0      94.0   \n4          4       2.5    3067.0       3.0       1.0  2.0     120.0   \n...      ...       ...       ...       ...       ...  ...       ...   \n13575      4      16.7    3150.0       4.0       2.0  2.0     652.0   \n13576      3       6.8    3016.0       3.0       2.0  2.0     333.0   \n13577      3       6.8    3016.0       3.0       2.0  4.0     436.0   \n13578      4       6.8    3016.0       4.0       1.0  5.0     866.0   \n13579      4       6.3    3013.0       4.0       1.0  1.0     362.0   \n\n       BuildingArea  YearBuilt  Lattitude  Longtitude  Propertycount  \n0               NaN        NaN  -37.79960   144.99840         4019.0  \n1              79.0     1900.0  -37.80790   144.99340         4019.0  \n2             150.0     1900.0  -37.80930   144.99440         4019.0  \n3               NaN        NaN  -37.79690   144.99690         4019.0  \n4             142.0     2014.0  -37.80720   144.99410         4019.0  \n...             ...        ...        ...         ...            ...  \n13575           NaN     1981.0  -37.90562   145.16761         7392.0  \n13576         133.0     1995.0  -37.85927   144.87904         6380.0  \n13577           NaN     1997.0  -37.85274   144.88738         6380.0  \n13578         157.0     1920.0  -37.85908   144.89299         6380.0  \n13579         112.0     1920.0  -37.81188   144.88449         6543.0  \n\n[13580 rows x 12 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Rooms</th>\n      <th>Distance</th>\n      <th>Postcode</th>\n      <th>Bedroom2</th>\n      <th>Bathroom</th>\n      <th>Car</th>\n      <th>Landsize</th>\n      <th>BuildingArea</th>\n      <th>YearBuilt</th>\n      <th>Lattitude</th>\n      <th>Longtitude</th>\n      <th>Propertycount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>2.5</td>\n      <td>3067.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>202.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-37.79960</td>\n      <td>144.99840</td>\n      <td>4019.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2.5</td>\n      <td>3067.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>156.0</td>\n      <td>79.0</td>\n      <td>1900.0</td>\n      <td>-37.80790</td>\n      <td>144.99340</td>\n      <td>4019.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>2.5</td>\n      <td>3067.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>134.0</td>\n      <td>150.0</td>\n      <td>1900.0</td>\n      <td>-37.80930</td>\n      <td>144.99440</td>\n      <td>4019.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>2.5</td>\n      <td>3067.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>94.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-37.79690</td>\n      <td>144.99690</td>\n      <td>4019.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2.5</td>\n      <td>3067.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>120.0</td>\n      <td>142.0</td>\n      <td>2014.0</td>\n      <td>-37.80720</td>\n      <td>144.99410</td>\n      <td>4019.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>13575</th>\n      <td>4</td>\n      <td>16.7</td>\n      <td>3150.0</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>652.0</td>\n      <td>NaN</td>\n      <td>1981.0</td>\n      <td>-37.90562</td>\n      <td>145.16761</td>\n      <td>7392.0</td>\n    </tr>\n    <tr>\n      <th>13576</th>\n      <td>3</td>\n      <td>6.8</td>\n      <td>3016.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>333.0</td>\n      <td>133.0</td>\n      <td>1995.0</td>\n      <td>-37.85927</td>\n      <td>144.87904</td>\n      <td>6380.0</td>\n    </tr>\n    <tr>\n      <th>13577</th>\n      <td>3</td>\n      <td>6.8</td>\n      <td>3016.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>436.0</td>\n      <td>NaN</td>\n      <td>1997.0</td>\n      <td>-37.85274</td>\n      <td>144.88738</td>\n      <td>6380.0</td>\n    </tr>\n    <tr>\n      <th>13578</th>\n      <td>4</td>\n      <td>6.8</td>\n      <td>3016.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>866.0</td>\n      <td>157.0</td>\n      <td>1920.0</td>\n      <td>-37.85908</td>\n      <td>144.89299</td>\n      <td>6380.0</td>\n    </tr>\n    <tr>\n      <th>13579</th>\n      <td>4</td>\n      <td>6.3</td>\n      <td>3013.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>362.0</td>\n      <td>112.0</td>\n      <td>1920.0</td>\n      <td>-37.81188</td>\n      <td>144.88449</td>\n      <td>6543.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>13580 rows × 12 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Split data into training and validation subsets**","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T20:13:53.690574Z","iopub.execute_input":"2023-07-24T20:13:53.691034Z","iopub.status.idle":"2023-07-24T20:13:53.702410Z","shell.execute_reply.started":"2023-07-24T20:13:53.690994Z","shell.execute_reply":"2023-07-24T20:13:53.701182Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# the data seems it's splited correctly\nprint(X_train)\nprint(y_train)\nprint(X_valid)\nprint(y_valid)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T20:13:53.703666Z","iopub.execute_input":"2023-07-24T20:13:53.704019Z","iopub.status.idle":"2023-07-24T20:13:53.742061Z","shell.execute_reply.started":"2023-07-24T20:13:53.703989Z","shell.execute_reply":"2023-07-24T20:13:53.741014Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"       Rooms  Distance  Postcode  Bedroom2  Bathroom  Car  Landsize  \\\n12167      1       5.0    3182.0       1.0       1.0  1.0       0.0   \n6524       2       8.0    3016.0       2.0       2.0  1.0     193.0   \n8413       3      12.6    3020.0       3.0       1.0  1.0     555.0   \n2919       3      13.0    3046.0       3.0       1.0  1.0     265.0   \n6043       3      13.3    3020.0       3.0       1.0  2.0     673.0   \n...      ...       ...       ...       ...       ...  ...       ...   \n13123      3       5.2    3056.0       3.0       1.0  2.0     212.0   \n3264       3      10.5    3081.0       3.0       1.0  1.0     748.0   \n9845       4       6.7    3058.0       4.0       2.0  2.0     441.0   \n10799      3      12.0    3073.0       3.0       1.0  1.0     606.0   \n2732       4       6.4    3011.0       4.0       2.0  1.0     319.0   \n\n       BuildingArea  YearBuilt  Lattitude  Longtitude  Propertycount  \n12167           NaN     1940.0  -37.85984   144.98670        13240.0  \n6524            NaN        NaN  -37.85800   144.90050         6380.0  \n8413            NaN        NaN  -37.79880   144.82200         3755.0  \n2919            NaN     1995.0  -37.70830   144.91580         8870.0  \n6043          673.0     1970.0  -37.76230   144.82720         4217.0  \n...             ...        ...        ...         ...            ...  \n13123           NaN        NaN  -37.77695   144.95785        11918.0  \n3264          101.0     1950.0  -37.74160   145.04810         2947.0  \n9845          255.0     2002.0  -37.73572   144.97256        11204.0  \n10799           NaN        NaN  -37.72057   145.02615        21650.0  \n2732          130.0     1915.0  -37.79430   144.88750         7570.0  \n\n[10864 rows x 12 columns]\n12167     481000.0\n6524      895000.0\n8413      651500.0\n2919      482500.0\n6043      591000.0\n           ...    \n13123    1280000.0\n3264      915000.0\n9845     1020000.0\n10799     760000.0\n2732     1225000.0\nName: Price, Length: 10864, dtype: float64\n       Rooms  Distance  Postcode  Bedroom2  Bathroom  Car  Landsize  \\\n8505       4       8.0    3016.0       4.0       2.0  2.0     450.0   \n5523       2       6.6    3011.0       2.0       1.0  0.0     172.0   \n12852      3      10.5    3020.0       3.0       1.0  1.0     581.0   \n4818       3       4.5    3181.0       2.0       2.0  1.0     128.0   \n12812      3       8.5    3044.0       3.0       2.0  2.0     480.0   \n...      ...       ...       ...       ...       ...  ...       ...   \n2664       2       6.4    3011.0       2.0       1.0  1.0      47.0   \n8513       4       8.0    3016.0       4.0       2.0  4.0     551.0   \n12922      3      10.8    3105.0       3.0       1.0  1.0     757.0   \n10761      4       6.2    3039.0       4.0       1.0  3.0     478.0   \n2110       2       1.6    3066.0       2.0       1.0  2.0     159.0   \n\n       BuildingArea  YearBuilt  Lattitude  Longtitude  Propertycount  \n8505          190.0     1910.0  -37.86100   144.89850         6380.0  \n5523           81.0     1900.0  -37.81000   144.88960         2417.0  \n12852           NaN        NaN  -37.76740   144.82421         4217.0  \n4818          134.0     2000.0  -37.85260   145.00710         7717.0  \n12812           NaN        NaN  -37.72523   144.94567         7485.0  \n...             ...        ...        ...         ...            ...  \n2664           35.0     2013.0  -37.80140   144.89590         7570.0  \n8513            NaN        NaN  -37.85790   144.87860         6380.0  \n12922           NaN        NaN  -37.78094   145.10131         4480.0  \n10761         152.0     1925.0  -37.76421   144.90571         6232.0  \n2110           86.0     1880.0  -37.79620   144.98870         4553.0  \n\n[2716 rows x 12 columns]\n8505     2165000.0\n5523      815000.0\n12852     610000.0\n4818     1245000.0\n12812    1160000.0\n           ...    \n2664      305000.0\n8513     1412000.0\n12922    1230000.0\n10761    1270000.0\n2110     1000000.0\nName: Price, Length: 2716, dtype: float64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Try Three approaches with the model and compare the prediction peformance of each model**","metadata":{}},{"cell_type":"code","source":"# Approach 1 - Dropping columns with NaN\n\n# What columns have missing values?\ncols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()] # if any element in the column is null, then True\nprint(\"The columns with missing values: \", cols_with_missing) \nprint(\"_\" * 70)\n\n# Drop the colums in both training and validation dataset\nreduced_X_train = X_train.drop(cols_with_missing, axis=1)\nreduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\nprint(reduced_X_train) \nprint(\"_\" * 70)\n\nprint(reduced_X_valid) \nprint(\"_\" * 70)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-24T20:13:53.743572Z","iopub.execute_input":"2023-07-24T20:13:53.743978Z","iopub.status.idle":"2023-07-24T20:13:53.775696Z","shell.execute_reply.started":"2023-07-24T20:13:53.743945Z","shell.execute_reply":"2023-07-24T20:13:53.774619Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"The columns with missing values:  ['Car', 'BuildingArea', 'YearBuilt']\n______________________________________________________________________\n       Rooms  Distance  Postcode  Bedroom2  Bathroom  Landsize  Lattitude  \\\n12167      1       5.0    3182.0       1.0       1.0       0.0  -37.85984   \n6524       2       8.0    3016.0       2.0       2.0     193.0  -37.85800   \n8413       3      12.6    3020.0       3.0       1.0     555.0  -37.79880   \n2919       3      13.0    3046.0       3.0       1.0     265.0  -37.70830   \n6043       3      13.3    3020.0       3.0       1.0     673.0  -37.76230   \n...      ...       ...       ...       ...       ...       ...        ...   \n13123      3       5.2    3056.0       3.0       1.0     212.0  -37.77695   \n3264       3      10.5    3081.0       3.0       1.0     748.0  -37.74160   \n9845       4       6.7    3058.0       4.0       2.0     441.0  -37.73572   \n10799      3      12.0    3073.0       3.0       1.0     606.0  -37.72057   \n2732       4       6.4    3011.0       4.0       2.0     319.0  -37.79430   \n\n       Longtitude  Propertycount  \n12167   144.98670        13240.0  \n6524    144.90050         6380.0  \n8413    144.82200         3755.0  \n2919    144.91580         8870.0  \n6043    144.82720         4217.0  \n...           ...            ...  \n13123   144.95785        11918.0  \n3264    145.04810         2947.0  \n9845    144.97256        11204.0  \n10799   145.02615        21650.0  \n2732    144.88750         7570.0  \n\n[10864 rows x 9 columns]\n______________________________________________________________________\n       Rooms  Distance  Postcode  Bedroom2  Bathroom  Landsize  Lattitude  \\\n8505       4       8.0    3016.0       4.0       2.0     450.0  -37.86100   \n5523       2       6.6    3011.0       2.0       1.0     172.0  -37.81000   \n12852      3      10.5    3020.0       3.0       1.0     581.0  -37.76740   \n4818       3       4.5    3181.0       2.0       2.0     128.0  -37.85260   \n12812      3       8.5    3044.0       3.0       2.0     480.0  -37.72523   \n...      ...       ...       ...       ...       ...       ...        ...   \n2664       2       6.4    3011.0       2.0       1.0      47.0  -37.80140   \n8513       4       8.0    3016.0       4.0       2.0     551.0  -37.85790   \n12922      3      10.8    3105.0       3.0       1.0     757.0  -37.78094   \n10761      4       6.2    3039.0       4.0       1.0     478.0  -37.76421   \n2110       2       1.6    3066.0       2.0       1.0     159.0  -37.79620   \n\n       Longtitude  Propertycount  \n8505    144.89850         6380.0  \n5523    144.88960         2417.0  \n12852   144.82421         4217.0  \n4818    145.00710         7717.0  \n12812   144.94567         7485.0  \n...           ...            ...  \n2664    144.89590         7570.0  \n8513    144.87860         6380.0  \n12922   145.10131         4480.0  \n10761   144.90571         6232.0  \n2110    144.98870         4553.0  \n\n[2716 rows x 9 columns]\n______________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Define a helper function to evaluate the model performance, to be used for each model with different approaches (handling missing values), using Random Forest model from scikit-learn, MAE method**","metadata":{}},{"cell_type":"code","source":"# modules\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# function to build a rnadom forest model and return MAE performance\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=10, random_state=0)   # create a model\n    model.fit(X_train, y_train)   # train the model\n    preds = model.predict(X_valid)      # model prediction\n    return mean_absolute_error(y_valid, preds)   # return MEA evaluation of real resut vs prediction","metadata":{"execution":{"iopub.status.busy":"2023-07-24T20:13:53.777130Z","iopub.execute_input":"2023-07-24T20:13:53.777989Z","iopub.status.idle":"2023-07-24T20:13:54.187793Z","shell.execute_reply.started":"2023-07-24T20:13:53.777953Z","shell.execute_reply":"2023-07-24T20:13:54.186724Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Approach 1 - Simply drop the columns with missing values\nprint(\"MAE from Approach 1 (Drop columns with missing values):\")\nprint(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))","metadata":{"execution":{"iopub.status.busy":"2023-07-24T20:13:54.189257Z","iopub.execute_input":"2023-07-24T20:13:54.189933Z","iopub.status.idle":"2023-07-24T20:13:54.752820Z","shell.execute_reply.started":"2023-07-24T20:13:54.189890Z","shell.execute_reply":"2023-07-24T20:13:54.751637Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"MAE from Approach 1 (Drop columns with missing values):\n183550.22137772635\n","output_type":"stream"}]},{"cell_type":"code","source":"# Approach 2 - Imputation - Fill the missing values with imputation 'strategy'\n# the process of filling in missing values with appropriate replacements, enabling the data to be used effectively in machine learning algorithms.\nfrom sklearn.impute import SimpleImputer\n\n# Imputation\nimputer = SimpleImputer() # default strategy='mean'\n\n# fit the imputer into the data and transform the data - train, valid dataset both\n# Be aware the df will loose column names after imputation\n# Make the imputed dataset as DataFrame, so you can restore columns to it, otherwise it will remain ans ndarray\nimputed_X_train = pd.DataFrame(imputer.fit_transform(X_train))\nimputed_X_valid = pd.DataFrame(imputer.transform(X_valid))  # apply the same imputer used with X_train\n#print(imputed_X_train.columns)\n#print(imputed_X_train)\n#print(imputed_X_valid.columns)\n#print(imputed_X_valid)\n\n# Restore the column names for the imputed data (ndarray)\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns\n#print(imputed_X_train.columns)\n#print(imputed_X_train)\n#print(imputed_X_valid.columns)\n#print(imputed_X_valid)\n\n# Evaluate the model using the helper function\nprint(\"MAE from Approach 2 (Imputation): \")\nprint(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))\n\n# Note that WApproach 2 has lower MAE than Approach 1, so Approach 2 performed better on this dataset","metadata":{"execution":{"iopub.status.busy":"2023-07-24T20:13:54.754301Z","iopub.execute_input":"2023-07-24T20:13:54.754719Z","iopub.status.idle":"2023-07-24T20:13:55.487713Z","shell.execute_reply.started":"2023-07-24T20:13:54.754689Z","shell.execute_reply":"2023-07-24T20:13:55.486504Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"MAE from Approach 2 (Imputation): \n178166.46269899711\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Note that Approach 2 has lower MAE than Approach 1, so Approach 2 performed better on this dataset**","metadata":{}},{"cell_type":"code","source":"# Approach 3 - Extension to Imputation - Apply Imputation as approach2, with indication for the columns that are imputed\n\n# Work with copies of the original datasets, Will add flag comlumns to them.\nX_train_plus = X_train.copy()\nX_valid_plus = X_valid.copy()\n\n# What columns are you imputing? Make new columns to mark them with Boolean value.\n# Any column with null value(NaN) will be imputed.\n# Make new colums for them, and mark with Boolean value\nfor col in cols_with_missing:\n    X_train_plus[col + '_was_missing'] = X_train[col].isnull()  # True if the cell(value) was null\n    X_valid_plus[col + '_was_missing'] = X_valid[col].isnull()  # True if the cell(value) was null\n\n# Check the relevant columns quickly and verify if the values are correct, side-by-side\nX_train_plus.loc[:,cols_with_missing + [col+'_was_missing' for col in cols_with_missing]]\n# The new columns look good.\n\n# Now impute the columns with missing values as Approach 2.\nimputer = SimpleImputer()\nimputed_X_train_plus = pd.DataFrame(imputer.fit_transform(X_train_plus))\nimputed_X_valid_plus = pd.DataFrame(imputer.fit_transform(X_valid_plus))\n\n# Restore columns to the new df, where lost during imputation\nimputed_X_train_plus.columns = X_train_plus.columns\nimputed_X_valid_plus.columns = X_valid_plus.columns\n\n# Check the new df's quickly\nimputed_X_train_plus\nimputed_X_valid_plus\n# Now see, no NaN columns and new flag columns with 1 or 0 values\n\n# Evaluate the model using the helper function\nprint(\"MAE from Approach 3 (An Extension to Imputation):\")\nprint(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))\n# Now see that Approach 3 performed slightly worse than Approach 2.\n    ","metadata":{"execution":{"iopub.status.busy":"2023-07-24T20:13:55.489821Z","iopub.execute_input":"2023-07-24T20:13:55.490288Z","iopub.status.idle":"2023-07-24T20:13:56.284541Z","shell.execute_reply.started":"2023-07-24T20:13:55.490240Z","shell.execute_reply":"2023-07-24T20:13:56.283204Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"MAE from Approach 3 (An Extension to Imputation):\n179986.2708570026\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Now see that Approach 3 performed slightly worse than Approach 2.**","metadata":{}},{"cell_type":"markdown","source":"**So, why did imputation perform better than dropping the columns?**    \n> The training data has 10864 rows and 12 columns, where three columns contain missing data. For each column, less than half of the entries are missing. Thus, dropping the columns removes a lot of useful information, and so it makes sense that imputation would perform better.","metadata":{}},{"cell_type":"markdown","source":"### As is common, imputing missing values (in Approach 2 and Approach 3) yielded better results, relative to when we simply dropped columns with missing values (in Approach 1).","metadata":{}},{"cell_type":"markdown","source":"**Exercise: Missing Values**","metadata":{}},{"cell_type":"markdown","source":"> ---\n> # Categorical Variables\n> ---\n \n* Only a limited number of values\n* Error with most machine learning models in Python without preprocesing them first\n* Three general approaches for preprocessing the column(s) with categorical variables\n    1. Drop categorical variables (the simplest, but the worst performace in general)\n    1. Ordianl encoding (Change the value of categorical variables to ranking-type integers, Tree-based models works well with this.\n    1. One Hot encoding (Add new columns with unique categorical values from the column, encode each with 1/0, like attaching a wide-pivot columns). To use when the categorical variables are norminal variables (not in intrinsic ranking). Recommended to use when cardinality is < 15\n* Work flow\n\nDataset to use: /kaggle/input/melbourne-housing-snapshot/melb_data.csv","metadata":{}},{"cell_type":"code","source":"# Load modules\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n\n\n# Load data\ndata = pd.read_csv(\"/kaggle/input/melbourne-housing-snapshot/melb_data.csv\")\n\n\n# Separate target from predictors (features)\ny = data['Price']  # target - he model will predict values for this column\nX = data.drop(['Price'], axis=1) # features - everything else except the target column\n\n\n# Split the data into training and validation subsets\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n\n\n# Preprocess - Drop columns with missing values \n  # 1. Get columns with missing values \ncols_with_missing = [ col for col in X_train_full.columns if X_train_full[col].isnull().any() ] # ['Car', 'BuildingArea', 'YearBuilt', 'CouncilArea']\n\n  # 2. Drop the columns (simplest approach applied here)\nX_train_full.drop(cols_with_missing, axis=1, inplace=True)\nX_valid_full.drop(cols_with_missing, axis=1, inplace=True)\n\n\n# Preprocess - construct data with columns in categorical or numerical values only\n\n  # 1. Get the categorical variable columns with low cardinality only (<10)\nlow_cardinality_cols = [ cname for cname in X_train_full.columns if X_train_full[cname].dtype =='object' \n                        and X_train_full[cname].nunique() < 10 ]  # ['Type', 'Method', 'Regionname']\n\n  # 2. Select numerical columns\nnumerical_cols = [ cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64'] ]  # ['Rooms','Distance','Postcode','Bedroom2','Bathroom','Car','Landsize','BuildingArea','YearBuilt','Lattitude','Longtitude','Propertycount']\n\n  # 3. Construct data with only the columns selected above - low cardinality columns + numerical colums\nmy_cols = low_cardinality_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\n\n\n# Check the final dataset for modeling\n#print(X_train.head(), \"\\n\")\n\n\n# Build, train and test a model applying the choice of categorical variable preprocessing approach\n# To define a helper funcdtion for model building, training, testing, measuring - Using Random Forest\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)\n\n\n# Try each approach of categorical variable preprosessing using the helper function\n\n# 1. Approach 1 (Drop Categorical Variables)\ndrop_X_train = X_train.select_dtypes(exclude=['object'])  # filter out the columns in string types and object types\ndrop_X_valid = X_valid.select_dtypes(exclude=['object'])  \n\nprint(\"1. MAE score from Approach 1 (Drop categorical variables):\")\nprint(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))\n\n# 2. Approach 2 (Ordinal Encoding)\n# Apply OrdinalEncoder to each categorical value column\nlabel_X_train = X_train.copy()\nlabel_X_valid = X_valid.copy()\n\nordinal_encoder = OrdinalEncoder()\nlabel_X_train[low_cardinality_cols] = ordinal_encoder.fit_transform(X_train[low_cardinality_cols])\nlabel_X_valid[low_cardinality_cols] = ordinal_encoder.transform(X_valid[low_cardinality_cols])\n\nprint(\"2. MAE score from Approach 2 (Ordinal Encoding:\")\nprint(score_dataset(label_X_train, label_X_valid, y_train, y_valid))\n\n# 3. Approach 3 (One-Hot Encoding)\n# Apply one-hot encoder to each column with categorical data\n# handle_unknown='ignore'to avoid errors when the validation data contains classes that aren't represented in the training data\n# sparse=False to return encoded columns as numpy arrays rather than sparse matrix\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n\n# Chceck the dataset for encoded columns\n#print(X_train[low_cardinality_cols])\n#print(OH_cols_train)\n#print(X_train.index)\n#print(OH_cols_train.index)\n\n# One-hot encoding removed index; put it back - the row labels\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(low_cardinality_cols, axis=1)\nnum_X_valid = X_valid.drop(low_cardinality_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features - Combine the numerical columsn + hot-encoded categorical columns\n# For final dataset for modeling and testing\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n\n# Ensure all columns have string type - One-Hot encoded columns with column names in number\nOH_X_train.columns = OH_X_train.columns.astype(str)\nOH_X_valid.columns = OH_X_valid.columns.astype(str)\n\nprint(\"3. MAE from Approach 3 (One-Hot Encoding):\") \nprint(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-24T20:13:56.289664Z","iopub.execute_input":"2023-07-24T20:13:56.290057Z","iopub.status.idle":"2023-07-24T20:14:14.769049Z","shell.execute_reply.started":"2023-07-24T20:13:56.290025Z","shell.execute_reply":"2023-07-24T20:14:14.767830Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"1. MAE score from Approach 1 (Drop categorical variables):\n175703.48185157913\n2. MAE score from Approach 2 (Ordinal Encoding:\n165936.40548390493\n3. MAE from Approach 3 (One-Hot Encoding):\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"166089.4893009678\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Which approach is best?\n### In this case, dropping the categorical columns (Approach 1) performed worst, since it had the highest MAE score. As for the other two approaches, since the returned MAE scores are so close in value, there doesn't appear to be any meaningful benefit to one over the other.\n \n### In general, one-hot encoding (Approach 3) will typically perform best, and dropping the categorical columns (Approach 1) typically performs worst, but it varies on a case-by-case basis.","metadata":{}},{"cell_type":"markdown","source":"**Exercises: Categorical Values**","metadata":{}},{"cell_type":"markdown","source":"> ---\n> # Pipelines\n> ---\n \n* Simple way to keep the codes organized by bundling the steps in processing + modeling\n* Cleaner Code\n* Fewer Bugs\n* Easier to Productionize\n* More OPtions for Model validation\n\nDataset to use: Melbourne Housing dataset\n/kaggle/input/melbourne-housing-snapshot/melb_data.csv\n\n### Pipelines are valuable for cleaning up machine learning code and avoiding errors, and are especially useful for workflows with sophisticated data preprocessing.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data \ndata = pd.read_csv(\"/kaggle/input/melbourne-housing-snapshot/melb_data.csv\")\n\n# Sparate target from predictors\ny = data.Price \nX = data.drop(['Price'], axis=1)\n\n# Divide data into training and validation subsets\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n\n# Extract columns in categorical / numerical values of your interest, they will be fed into the model of your choice.\n# Conversion of categorical values to numerics are crucial since most of the pre-defined models are based on to \n# use numeric values.\n# \n# Categorical columns with low Cardinality (<10)\ncategorical_cols = [ col for col in X_train_full.columns if X_train_full[col].nunique() < 10 \n                   and X_train_full[col].dtype == 'object' ]\nprint(categorical_cols, len(categorical_cols))\n\n# Numerical columns\nnumerical_cols = [ col for col in X_train_full.columns if X_train_full[col].dtype in ['int64','float64']]\nprint(numerical_cols, len(numerical_cols))\n\n# Filter dataset with the selected column names\nmy_cols = categorical_cols + numerical_cols\nprint(my_cols, len(my_cols))\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-24T20:14:14.770891Z","iopub.execute_input":"2023-07-24T20:14:14.771343Z","iopub.status.idle":"2023-07-24T20:14:14.882575Z","shell.execute_reply.started":"2023-07-24T20:14:14.771301Z","shell.execute_reply":"2023-07-24T20:14:14.881418Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"['Type', 'Method', 'Regionname'] 3\n['Rooms', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount'] 12\n['Type', 'Method', 'Regionname', 'Rooms', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount'] 15\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-24T20:14:14.884376Z","iopub.execute_input":"2023-07-24T20:14:14.885218Z","iopub.status.idle":"2023-07-24T20:14:14.908400Z","shell.execute_reply.started":"2023-07-24T20:14:14.885179Z","shell.execute_reply":"2023-07-24T20:14:14.907238Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"      Type Method             Regionname  Rooms  Distance  Postcode  Bedroom2  \\\n12167    u      S  Southern Metropolitan      1       5.0    3182.0       1.0   \n6524     h     SA   Western Metropolitan      2       8.0    3016.0       2.0   \n8413     h      S   Western Metropolitan      3      12.6    3020.0       3.0   \n2919     u     SP  Northern Metropolitan      3      13.0    3046.0       3.0   \n6043     h      S   Western Metropolitan      3      13.3    3020.0       3.0   \n\n       Bathroom  Car  Landsize  BuildingArea  YearBuilt  Lattitude  \\\n12167       1.0  1.0       0.0           NaN     1940.0  -37.85984   \n6524        2.0  1.0     193.0           NaN        NaN  -37.85800   \n8413        1.0  1.0     555.0           NaN        NaN  -37.79880   \n2919        1.0  1.0     265.0           NaN     1995.0  -37.70830   \n6043        1.0  2.0     673.0         673.0     1970.0  -37.76230   \n\n       Longtitude  Propertycount  \n12167    144.9867        13240.0  \n6524     144.9005         6380.0  \n8413     144.8220         3755.0  \n2919     144.9158         8870.0  \n6043     144.8272         4217.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Type</th>\n      <th>Method</th>\n      <th>Regionname</th>\n      <th>Rooms</th>\n      <th>Distance</th>\n      <th>Postcode</th>\n      <th>Bedroom2</th>\n      <th>Bathroom</th>\n      <th>Car</th>\n      <th>Landsize</th>\n      <th>BuildingArea</th>\n      <th>YearBuilt</th>\n      <th>Lattitude</th>\n      <th>Longtitude</th>\n      <th>Propertycount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12167</th>\n      <td>u</td>\n      <td>S</td>\n      <td>Southern Metropolitan</td>\n      <td>1</td>\n      <td>5.0</td>\n      <td>3182.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>1940.0</td>\n      <td>-37.85984</td>\n      <td>144.9867</td>\n      <td>13240.0</td>\n    </tr>\n    <tr>\n      <th>6524</th>\n      <td>h</td>\n      <td>SA</td>\n      <td>Western Metropolitan</td>\n      <td>2</td>\n      <td>8.0</td>\n      <td>3016.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>193.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-37.85800</td>\n      <td>144.9005</td>\n      <td>6380.0</td>\n    </tr>\n    <tr>\n      <th>8413</th>\n      <td>h</td>\n      <td>S</td>\n      <td>Western Metropolitan</td>\n      <td>3</td>\n      <td>12.6</td>\n      <td>3020.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>555.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-37.79880</td>\n      <td>144.8220</td>\n      <td>3755.0</td>\n    </tr>\n    <tr>\n      <th>2919</th>\n      <td>u</td>\n      <td>SP</td>\n      <td>Northern Metropolitan</td>\n      <td>3</td>\n      <td>13.0</td>\n      <td>3046.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>265.0</td>\n      <td>NaN</td>\n      <td>1995.0</td>\n      <td>-37.70830</td>\n      <td>144.9158</td>\n      <td>8870.0</td>\n    </tr>\n    <tr>\n      <th>6043</th>\n      <td>h</td>\n      <td>S</td>\n      <td>Western Metropolitan</td>\n      <td>3</td>\n      <td>13.3</td>\n      <td>3020.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>673.0</td>\n      <td>673.0</td>\n      <td>1970.0</td>\n      <td>-37.76230</td>\n      <td>144.8272</td>\n      <td>4217.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Construct full pipeline in Three Steps**","metadata":{}},{"cell_type":"markdown","source":"### **STEP 1: Define Preprocessing Steps - Transformers|**   \nUse **ColumnTransformer** class to bundle different preprocessing steps","metadata":{}},{"cell_type":"code","source":"# Scenario\n# 1. Impute missing values in numerical data\n# 2. Impute missing values and applies a one-hot encoding to categorical data\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Prepocessing for numerical data\n# Use this transfomer as defined to process the null values in numerical values\nnumerical_transformer = SimpleImputer(strategy='constant')  # default = None\n\n# Preprocessing for categorical data\n# Use this transformer as defined to process the null values in categorical values and convert them into numberical values\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\n# Define the preprocessor for model by bundling the preprocessors defined abovve, Will be used to build a model.\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n","metadata":{"execution":{"iopub.status.busy":"2023-07-24T20:14:14.910124Z","iopub.execute_input":"2023-07-24T20:14:14.910463Z","iopub.status.idle":"2023-07-24T20:14:14.938001Z","shell.execute_reply.started":"2023-07-24T20:14:14.910433Z","shell.execute_reply":"2023-07-24T20:14:14.936725Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## STEP2: Bundle transformers and define a preprocessor for the model","metadata":{}},{"cell_type":"code","source":"# Bundle preprocessing for numberical and categorical data\n# - ColumnTransformer Class\nColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T20:14:21.524257Z","iopub.execute_input":"2023-07-24T20:14:21.524683Z","iopub.status.idle":"2023-07-24T20:14:21.560242Z","shell.execute_reply.started":"2023-07-24T20:14:21.524648Z","shell.execute_reply":"2023-07-24T20:14:21.559149Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"ColumnTransformer(transformers=[('num', SimpleImputer(strategy='constant'),\n                                 ['Rooms', 'Distance', 'Postcode', 'Bedroom2',\n                                  'Bathroom', 'Car', 'Landsize', 'BuildingArea',\n                                  'YearBuilt', 'Lattitude', 'Longtitude',\n                                  'Propertycount']),\n                                ('cat',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('onehot',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['Type', 'Method', 'Regionname'])])","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;, SimpleImputer(strategy=&#x27;constant&#x27;),\n                                 [&#x27;Rooms&#x27;, &#x27;Distance&#x27;, &#x27;Postcode&#x27;, &#x27;Bedroom2&#x27;,\n                                  &#x27;Bathroom&#x27;, &#x27;Car&#x27;, &#x27;Landsize&#x27;, &#x27;BuildingArea&#x27;,\n                                  &#x27;YearBuilt&#x27;, &#x27;Lattitude&#x27;, &#x27;Longtitude&#x27;,\n                                  &#x27;Propertycount&#x27;]),\n                                (&#x27;cat&#x27;,\n                                 Pipeline(steps=[(&#x27;imputer&#x27;,\n                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n                                                 (&#x27;onehot&#x27;,\n                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n                                 [&#x27;Type&#x27;, &#x27;Method&#x27;, &#x27;Regionname&#x27;])])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;, SimpleImputer(strategy=&#x27;constant&#x27;),\n                                 [&#x27;Rooms&#x27;, &#x27;Distance&#x27;, &#x27;Postcode&#x27;, &#x27;Bedroom2&#x27;,\n                                  &#x27;Bathroom&#x27;, &#x27;Car&#x27;, &#x27;Landsize&#x27;, &#x27;BuildingArea&#x27;,\n                                  &#x27;YearBuilt&#x27;, &#x27;Lattitude&#x27;, &#x27;Longtitude&#x27;,\n                                  &#x27;Propertycount&#x27;]),\n                                (&#x27;cat&#x27;,\n                                 Pipeline(steps=[(&#x27;imputer&#x27;,\n                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n                                                 (&#x27;onehot&#x27;,\n                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n                                 [&#x27;Type&#x27;, &#x27;Method&#x27;, &#x27;Regionname&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Rooms&#x27;, &#x27;Distance&#x27;, &#x27;Postcode&#x27;, &#x27;Bedroom2&#x27;, &#x27;Bathroom&#x27;, &#x27;Car&#x27;, &#x27;Landsize&#x27;, &#x27;BuildingArea&#x27;, &#x27;YearBuilt&#x27;, &#x27;Lattitude&#x27;, &#x27;Longtitude&#x27;, &#x27;Propertycount&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;constant&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Type&#x27;, &#x27;Method&#x27;, &#x27;Regionname&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## STEP 3: Define your Model to use","metadata":{}},{"cell_type":"code","source":"# Will use Randome Forest Model - RandomForestRegressor (in this code base)\nfrom sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T20:21:59.163553Z","iopub.execute_input":"2023-07-24T20:21:59.164470Z","iopub.status.idle":"2023-07-24T20:21:59.169103Z","shell.execute_reply.started":"2023-07-24T20:21:59.164424Z","shell.execute_reply":"2023-07-24T20:21:59.167878Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Create a Pipeline - Combine the bundled preprosessor and the model - Pipeline Class\n# ** With the pipeline, we preprocess the training data and fit the model in a single line of code. \n# (In contrast, without a pipeline, we have to do imputation, one-hot encoding, and model training in separate steps. \n# This becomes especially messy if we have to deal with both numerical and categorical variables!)\n# ** With the pipeline, we supply the unprocessed features in X_valid to the predict() command, and \n# the pipeline automatically preprocesses the features before generating predictions. (However, without a pipeline, \n# we have to remember to preprocess the validation data before making predictions.)\n\n\nfrom sklearn.metrics import mean_absolute_error\n\n# Bundling preprocessing + modeling code in a pipline\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                           ('model', model)\n                          ])\n\n# Preprocessing of training data, fit model - feed the 'untrained', 'unprocessed' raw data to the Pipeline\npipeline.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions - feed the 'validation' data to the Pipeline\npreds = pipeline.predict(X_valid)\n\n# Evaluate the model - MAE, mean_ablouste_error (in this code base)\nscore = mean_absolute_error(y_valid, preds)\nprint('MAE:', score)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-24T20:29:51.603678Z","iopub.execute_input":"2023-07-24T20:29:51.604101Z","iopub.status.idle":"2023-07-24T20:29:59.775030Z","shell.execute_reply.started":"2023-07-24T20:29:51.604069Z","shell.execute_reply":"2023-07-24T20:29:59.773928Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"MAE: 160679.18917034855\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}